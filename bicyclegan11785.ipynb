{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGiE_azRyatN"
   },
   "source": [
    "# 11785 Fall 2024 Project: GAN-based Cross-Modality Medical Image Synthesis for Prostate Cancer \n",
    "\n",
    "# Group XV (Fifteen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbDqKWbfg-AI"
   },
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:31:47.166858Z",
     "iopub.status.busy": "2024-12-10T19:31:47.165871Z",
     "iopub.status.idle": "2024-12-10T19:32:30.775283Z",
     "shell.execute_reply": "2024-12-10T19:32:30.774297Z",
     "shell.execute_reply.started": "2024-12-10T19:31:47.166810Z"
    },
    "id": "AhSmnLI0yo-7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# install libraries\n",
    "!pip install torch torchmetrics --q\n",
    "!pip install wandb --quiet\n",
    "!pip install pytorch-msssim --q\n",
    "!pip install adabelief-pytorch --q\n",
    "\n",
    "!pip install torchsummary -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNDJLeigYU3t"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:30.777622Z",
     "iopub.status.busy": "2024-12-10T19:32:30.777317Z",
     "iopub.status.idle": "2024-12-10T19:32:35.480235Z",
     "shell.execute_reply": "2024-12-10T19:32:35.479323Z",
     "shell.execute_reply.started": "2024-12-10T19:32:30.777594Z"
    },
    "id": "xyP6fCaSyQEW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_msssim import ssim\n",
    "import random\n",
    "\n",
    "from adabelief_pytorch import AdaBelief\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keDXwxGQhHYD"
   },
   "source": [
    "# Check and Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:35.482436Z",
     "iopub.status.busy": "2024-12-10T19:32:35.481608Z",
     "iopub.status.idle": "2024-12-10T19:32:36.599683Z",
     "shell.execute_reply": "2024-12-10T19:32:36.598562Z",
     "shell.execute_reply.started": "2024-12-10T19:32:35.482394Z"
    },
    "id": "VcYUbuIGb9uY",
    "outputId": "e12a8202-196c-4bc1-b5e6-3ab6cb074634",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR-_MHIeachp"
   },
   "source": [
    "# Unzip Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:36.602440Z",
     "iopub.status.busy": "2024-12-10T19:32:36.601514Z",
     "iopub.status.idle": "2024-12-10T19:32:36.606423Z",
     "shell.execute_reply": "2024-12-10T19:32:36.605493Z",
     "shell.execute_reply.started": "2024-12-10T19:32:36.602396Z"
    },
    "id": "VLMcRlc-ytmC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# unzip dataset\n",
    "!unzip -q /content/ProstateCancerDataset.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SuzmJwUahhQ"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:36.609168Z",
     "iopub.status.busy": "2024-12-10T19:32:36.608838Z",
     "iopub.status.idle": "2024-12-10T19:32:36.619260Z",
     "shell.execute_reply": "2024-12-10T19:32:36.618552Z",
     "shell.execute_reply.started": "2024-12-10T19:32:36.609134Z"
    },
    "id": "Ma8QtQESzCyd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# configs\n",
    "config ={\n",
    "    \"run_name\" : 'bgan-sa-perc-gradpen-v11',\n",
    "    \"dataset_dir\" : 'ProstateCancerDataset',\n",
    "    \"output_dir\" : 'output_bgan',\n",
    "\n",
    "    \"epochs\" : 300,\n",
    "    \"img_size\": 256,\n",
    "\n",
    "    \"lr\": 0.0001,\n",
    "\n",
    "    #if using SGD (TTUR) vary the learning rates according to this:\n",
    "    \"lr_g\": 0.0001,         # Generator (slowest)\n",
    "    \"lr_e\": 0.0002,         # Encoder (between G and D rates)\n",
    "    \"lr_d\": 0.0004,         # Discriminator (fastest)\n",
    "\n",
    "    \"batch_size\" : 1,       # typically 1\n",
    "    \"num_workers\" : 4,\n",
    "    \"random_seed\": 0,\n",
    "\n",
    "    \"latent_dim\": 256,      # dimension of the latent vector, adjust based on desired diversity and complexity\n",
    "\n",
    "    \"lambda_L1\": 10,        # weight for L1 loss, adjust based on image sharpness\n",
    "    \"lambda_KL\": 0.001,     # Weight for KL divergence, adjust based on latent space regularization\n",
    "    \"grad_penalty_weight\": 10.0,\n",
    "    \"perceptual_loss_weight\": 1.0,\n",
    "\n",
    "    \"max_grad_norm\": 0.1,\n",
    "\n",
    "    \"transforms\": \"default\", # \"default\", \"augmentation\"\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEH1wIOuaq89"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:36.620512Z",
     "iopub.status.busy": "2024-12-10T19:32:36.620251Z",
     "iopub.status.idle": "2024-12-10T19:32:36.631087Z",
     "shell.execute_reply": "2024-12-10T19:32:36.630253Z",
     "shell.execute_reply.started": "2024-12-10T19:32:36.620487Z"
    },
    "id": "0sMQJKxnStg2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, transform=None, img_size=256):\n",
    "        self.root_dir = dataset_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        # create resize transform\n",
    "        self.resize = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size), antialias=True),\n",
    "        ])\n",
    "\n",
    "        # collect all slices\n",
    "        self.slices = []\n",
    "\n",
    "        # Scan directories\n",
    "        adc_dir = os.path.join(dataset_dir, \"ADC\")\n",
    "        for patient_dir in sorted(os.listdir(adc_dir)):\n",
    "            patient_adc_dir = os.path.join(adc_dir, patient_dir)\n",
    "            patient_t2w_dir = os.path.join(dataset_dir, \"T2w_Type\",\n",
    "                                         patient_dir.replace('_adc', '_t2w'))\n",
    "\n",
    "            if os.path.exists(patient_t2w_dir):\n",
    "                adc_files = sorted([f for f in os.listdir(patient_adc_dir)\n",
    "                                  if os.path.isfile(os.path.join(patient_adc_dir, f))])\n",
    "                t2w_files = sorted([f for f in os.listdir(patient_t2w_dir)\n",
    "                                  if os.path.isfile(os.path.join(patient_t2w_dir, f))])\n",
    "\n",
    "                min_slices = min(len(adc_files), len(t2w_files))\n",
    "\n",
    "                for i in range(min_slices):\n",
    "                    self.slices.append({\n",
    "                        'adc_path': os.path.join(patient_adc_dir, adc_files[i]),\n",
    "                        't2w_path': os.path.join(patient_t2w_dir, t2w_files[i])\n",
    "                    })\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slice_paths = self.slices[idx]\n",
    "\n",
    "        # load images\n",
    "        adc_image = Image.open(slice_paths['adc_path']).convert('L')\n",
    "        t2w_image = Image.open(slice_paths['t2w_path']).convert('L')\n",
    "\n",
    "        # resize images\n",
    "        adc_image = self.resize(adc_image)\n",
    "        t2w_image = self.resize(t2w_image)\n",
    "\n",
    "        # apply other transforms\n",
    "        if self.transform:\n",
    "            adc_image = self.transform(adc_image)\n",
    "            t2w_image = self.transform(t2w_image)\n",
    "\n",
    "        return {\n",
    "            'A': adc_image,  # source domain (ADC)\n",
    "            'B': t2w_image   # target domain (T2w)\n",
    "        }\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:36.632450Z",
     "iopub.status.busy": "2024-12-10T19:32:36.632159Z",
     "iopub.status.idle": "2024-12-10T19:32:36.648024Z",
     "shell.execute_reply": "2024-12-10T19:32:36.647219Z",
     "shell.execute_reply.started": "2024-12-10T19:32:36.632425Z"
    },
    "id": "FsMTZ77s5yzY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_data_loaders(config):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test data loaders\n",
    "    \"\"\"\n",
    "    # define transformations\n",
    "    if config[\"transforms\"] == \"default\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "    elif config[\"transforms\"] == \"augmentation\":\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "\n",
    "    # create full dataset\n",
    "    full_dataset = MRIDataset(dataset_dir=config['dataset_dir'], transform=transform)\n",
    "\n",
    "    # calculate lengths for splits\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(0.78 * total_size)\n",
    "    val_size = int(0.12 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # create splits\n",
    "    train_indices, temp_indices = train_test_split(\n",
    "        range(total_size), test_size=(val_size + test_size),\n",
    "        random_state=config['random_seed']\n",
    "    )\n",
    "    val_indices, test_indices = train_test_split(\n",
    "        temp_indices, test_size=test_size,\n",
    "        random_state=config['random_seed']\n",
    "    )\n",
    "\n",
    "    # create subset datasets based on split ratios\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "    test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "    # create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(f\"Total slices: {total_size}\")\n",
    "    print(f\"Training slices: {len(train_dataset)}\")\n",
    "    print(f\"Validation slices: {len(val_dataset)}\")\n",
    "    print(f\"Testing slices: {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:36.649341Z",
     "iopub.status.busy": "2024-12-10T19:32:36.649075Z",
     "iopub.status.idle": "2024-12-10T19:32:37.110948Z",
     "shell.execute_reply": "2024-12-10T19:32:37.110062Z",
     "shell.execute_reply.started": "2024-12-10T19:32:36.649316Z"
    },
    "id": "nPhFouu253Zq",
    "outputId": "3fb6cd86-78a2-4604-e814-75bf341f79cf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "train_loader, val_loader, test_loader = create_data_loaders(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGb-eDIhh5EG"
   },
   "source": [
    "# Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:37.112486Z",
     "iopub.status.busy": "2024-12-10T19:32:37.112065Z",
     "iopub.status.idle": "2024-12-10T19:32:41.854113Z",
     "shell.execute_reply": "2024-12-10T19:32:41.853252Z",
     "shell.execute_reply.started": "2024-12-10T19:32:37.112455Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:02<00:00, 212MB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FeatureExtractor(\n",
       "  (features): ModuleList(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# define a feature extractor \n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # use a pre-trained VGG model\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.selected_layers = layers\n",
    "        self.features = nn.ModuleList([vgg[i] for i in layers]).eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            if i in self.selected_layers:\n",
    "                outputs.append(x)\n",
    "        return outputs\n",
    "\n",
    "# convert single-channel images to 3-channel\n",
    "def to_three_channels(img):\n",
    "    return img.repeat(1, 3, 1, 1)  # Repeat along the channel dimension\n",
    "\n",
    "def perceptual_loss(real, fake, feature_extractor):\n",
    "    # convert grayscale images to 3-channel\n",
    "    real = to_three_channels(real)\n",
    "    fake = to_three_channels(fake)\n",
    "\n",
    "    # move images to the same device as the feature extractor\n",
    "    real = real.to(next(feature_extractor.parameters()).device)\n",
    "    fake = fake.to(next(feature_extractor.parameters()).device)\n",
    "\n",
    "    # compute feature maps\n",
    "    real_features = feature_extractor(real)\n",
    "    fake_features = feature_extractor(fake)\n",
    "\n",
    "    # compute L1 loss between feature maps\n",
    "    criterion = nn.L1Loss()\n",
    "    loss = 0\n",
    "    for r, f in zip(real_features, fake_features):\n",
    "        loss += criterion(r, f)\n",
    "    return loss\n",
    "\n",
    "\n",
    "feature_extractor = FeatureExtractor(layers=[0, 5, 10, 19]).to(device)  \n",
    "feature_extractor.eval()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:41.855378Z",
     "iopub.status.busy": "2024-12-10T19:32:41.855105Z",
     "iopub.status.idle": "2024-12-10T19:32:41.861674Z",
     "shell.execute_reply": "2024-12-10T19:32:41.860894Z",
     "shell.execute_reply.started": "2024-12-10T19:32:41.855353Z"
    },
    "id": "qx4h91SHiBqs",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# gradient penalty\n",
    "def gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    batch_size = real_samples.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1).to(real_samples.device)\n",
    "\n",
    "    # split concatenated samples back into source and target\n",
    "    # each sample has 2 channels (concatenated source and target)\n",
    "    source = real_samples[:, :1]  # first channel\n",
    "    interpolated_target = alpha * real_samples[:, 1:] + (1 - alpha) * fake_samples[:, 1:]\n",
    "    interpolated_target.requires_grad_(True)\n",
    "\n",
    "    # forward pass\n",
    "    d_interpolates = discriminator(source, interpolated_target)\n",
    "\n",
    "    # compute gradients\n",
    "    fake = torch.ones_like(d_interpolates).to(real_samples.device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolated_target,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "\n",
    "    # calculate gradient penalty\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLM9ToZRj2yD"
   },
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EPCUf8_j5LI"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:41.863342Z",
     "iopub.status.busy": "2024-12-10T19:32:41.862982Z",
     "iopub.status.idle": "2024-12-10T19:32:41.908302Z",
     "shell.execute_reply": "2024-12-10T19:32:41.907364Z",
     "shell.execute_reply.started": "2024-12-10T19:32:41.863305Z"
    },
    "id": "hc60RfUThZMo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # adjust projection dimensions to match input channels\n",
    "        self.query = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        # project queries, keys and values\n",
    "        proj_query = self.query(x).view(batch_size, -1, height * width).permute(0, 2, 1)  # B x HW x C\n",
    "        proj_key = self.key(x).view(batch_size, -1, height * width)  # B x C x HW\n",
    "        proj_value = self.value(x).view(batch_size, -1, height * width)  # B x C x HW\n",
    "\n",
    "        # attention map\n",
    "        energy = torch.bmm(proj_query, proj_key)  # B x HW x HW\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "\n",
    "        # apply attention to values\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # B x C x HW\n",
    "        out = out.view(batch_size, channels, height, width)\n",
    "\n",
    "        return self.gamma * out + x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # initial layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "        # downsampling layers with residual connections\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            self._make_down_block(64, 128),    # 128x128 -> 64x64\n",
    "            self._make_down_block(128, 256),   # 64x64 -> 32x32\n",
    "            self._make_down_block(256, 512),   # 32x32 -> 16x16\n",
    "            self._make_down_block(512, 512),   # 16x16 -> 8x8\n",
    "            self._make_down_block(512, 512)    # 8x8 -> 4x4\n",
    "        ])\n",
    "\n",
    "        # attention modules - match channel dimensions\n",
    "        self.attention_blocks = nn.ModuleList([\n",
    "            SelfAttention(512),  # after 3rd down block\n",
    "            SelfAttention(512),  # after 4th down block\n",
    "            SelfAttention(512)   # after 5th down block\n",
    "        ])\n",
    "\n",
    "        # output layers for mu and logvar with dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        self.fc_logvar = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "\n",
    "    def _make_down_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initial convolution\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # down blocks with attention\n",
    "        features = []\n",
    "        for i, block in enumerate(self.down_blocks):\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "            # apply attention only after reaching 512 channels (i >= 2)\n",
    "            if i >= 2:\n",
    "                x = self.attention_blocks[i-2](x)\n",
    "\n",
    "        # flatten and get latent parameters\n",
    "        x = self.dropout(self.flatten(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGxnmKvdkAOJ"
   },
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:41.909837Z",
     "iopub.status.busy": "2024-12-10T19:32:41.909553Z",
     "iopub.status.idle": "2024-12-10T19:32:41.925098Z",
     "shell.execute_reply": "2024-12-10T19:32:41.924349Z",
     "shell.execute_reply.started": "2024-12-10T19:32:41.909810Z"
    },
    "id": "eJGlKv3zkODU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # initial processing of source image\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, stride=2, padding=1),  # B x 64 x 128 x 128\n",
    "            nn.LeakyReLU(0.3, True),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        # downsample blocks\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            self._make_down_block(64, 128),    # B x 128 x 64 x 64\n",
    "            self._make_down_block(128, 256),   # B x 256 x 32 x 32\n",
    "            self._make_down_block(256, 512),   # B x 512 x 16 x 16\n",
    "            self._make_down_block(512, 512),   # B x 512 x 8 x 8\n",
    "            self._make_down_block(512, 512),   # B x 512 x 4 x 4\n",
    "        ])\n",
    "\n",
    "        # inject latent code\n",
    "        self.latent_projection = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512 * 4 * 4),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # upsample blocks\n",
    "        self.up_blocks = nn.ModuleList([\n",
    "            self._make_up_block(1024, 512),    # B x 512 x 8 x 8\n",
    "            self._make_up_block(1024, 512),    # B x 512 x 16 x 16\n",
    "            self._make_up_block(1024, 256),    # B x 256 x 32 x 32\n",
    "            self._make_up_block(512, 128),     # B x 128 x 64 x 64\n",
    "            self._make_up_block(256, 64),      # B x 64 x 128 x 128\n",
    "        ])\n",
    "\n",
    "        # final output layer\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 1, 4, stride=2, padding=1),  # B x 1 x 256 x 256\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def _make_down_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.3, True),\n",
    "        )\n",
    "\n",
    "    def _make_up_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        # process input image through downsample blocks\n",
    "        features = [self.conv1(x)]\n",
    "        for block in self.down_blocks:\n",
    "            features.append(block(features[-1]))\n",
    "\n",
    "        # process latent code\n",
    "        z = self.latent_projection(z)\n",
    "        z = z.view(z.size(0), -1, 4, 4)\n",
    "\n",
    "        # combine with last feature map\n",
    "        x = torch.cat([features[-1], z], dim=1)\n",
    "\n",
    "        # upsample with skip connections\n",
    "        for i, block in enumerate(self.up_blocks):\n",
    "            x = block(x)\n",
    "            if i < len(features) - 1:  # Skip connection\n",
    "                x = torch.cat([x, features[-(i+2)]], dim=1)\n",
    "\n",
    "        # final output\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGFCHGEmkQ_E"
   },
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:41.926322Z",
     "iopub.status.busy": "2024-12-10T19:32:41.926052Z",
     "iopub.status.idle": "2024-12-10T19:32:41.942277Z",
     "shell.execute_reply": "2024-12-10T19:32:41.941430Z",
     "shell.execute_reply.started": "2024-12-10T19:32:41.926297Z"
    },
    "id": "0ZS0bikKkURQ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # process paired images (source + target)\n",
    "        self.model = nn.Sequential(\n",
    "            # input: B x 2 x 256 x 256 (concatenated source and target images)\n",
    "            nn.Conv2d(2, 64, 4, stride=2, padding=1),  # B x 64 x 128 x 128\n",
    "            nn.LeakyReLU(0.3, True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # B x 128 x 64 x 64\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.3, True),\n",
    "\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),  # B x 256 x 32 x 32\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.3, True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),  # B x 512 x 16 x 16\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.3, True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, stride=1, padding=1),  # B x 1 x 15 x 15\n",
    "        )\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        # concatenate source and target images along channel dimension\n",
    "        x = torch.cat([source, target], dim=1)\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8d8DNnOks-A"
   },
   "source": [
    "## Bicycle GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:41.946572Z",
     "iopub.status.busy": "2024-12-10T19:32:41.946297Z",
     "iopub.status.idle": "2024-12-10T19:32:41.990122Z",
     "shell.execute_reply": "2024-12-10T19:32:41.989230Z",
     "shell.execute_reply.started": "2024-12-10T19:32:41.946546Z"
    },
    "id": "NCXeoLoskrVB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class that manages all models (E, G, D) and their interactions\n",
    "class BicycleGAN:\n",
    "    def __init__(self):\n",
    "        self.latent_dim = config['latent_dim']\n",
    "        self.input_size = config['img_size']\n",
    "        self.lambda_L1 = config['lambda_L1']\n",
    "        self.lambda_KL = config['lambda_KL']\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # initialize networks with correct input size\n",
    "        self.E = Encoder(latent_dim=config['latent_dim']).to(self.device)\n",
    "        self.G = Generator(latent_dim=config['latent_dim']).to(self.device)\n",
    "        self.D = Discriminator().to(self.device)\n",
    "\n",
    "        # initialize weights\n",
    "        self.E.apply(self.weights_init)\n",
    "        self.G.apply(self.weights_init)\n",
    "        self.D.apply(self.weights_init)\n",
    "\n",
    "        # initialize optimizers\n",
    "        self.opt_G = AdaBelief(\n",
    "            self.G.parameters(),\n",
    "            lr=config['lr_g'],\n",
    "            eps=1e-16,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decouple=True,\n",
    "            rectify=False,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "\n",
    "        self.opt_E = AdaBelief(\n",
    "            self.E.parameters(),\n",
    "            lr=config['lr_e'],\n",
    "            eps=1e-16,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decouple=True,\n",
    "            rectify=False,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "\n",
    "        self.opt_D = AdaBelief(\n",
    "            self.D.parameters(),\n",
    "            lr=config['lr_d'],\n",
    "            eps=1e-16,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decouple=True,\n",
    "            rectify=False,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "\n",
    "\n",
    "        # initialize schedulers\n",
    "        self.scheduler_G = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.opt_G,\n",
    "            mode='min',\n",
    "            factor=0.8,\n",
    "            patience=4,\n",
    "            verbose=True,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "\n",
    "        self.scheduler_E = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.opt_E,\n",
    "            mode='min',\n",
    "            factor=0.4,\n",
    "            patience=7,\n",
    "            verbose=True,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "\n",
    "        self.scheduler_D = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.opt_D,\n",
    "            mode='min',\n",
    "            factor=0.8,\n",
    "            patience=5,\n",
    "            verbose=True,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "\n",
    "        # define loss functions\n",
    "        self.criterion_GAN = nn.MSELoss()\n",
    "        self.criterion_L1 = nn.L1Loss()\n",
    "        self.criterion_KL = lambda mu, logvar: -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(m):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    def train_step(self, source, target, lambda_L1=10, lambda_KL=0.01):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        # add gradient clipping value\n",
    "        max_grad_norm = config['max_grad_norm']\n",
    "\n",
    "        real_label = torch.ones((source.size(0), 1, 15, 15)).to(self.device)\n",
    "        fake_label = torch.zeros((source.size(0), 1, 15, 15)).to(self.device)\n",
    "\n",
    "        # Fforward cycle (cVAE-GAN)\n",
    "        self.opt_E.zero_grad()\n",
    "        self.opt_G.zero_grad()\n",
    "\n",
    "        # get latent encoding of target image\n",
    "        mu, logvar = self.E(target)\n",
    "        z = self.E.reparameterize(mu, logvar)\n",
    "\n",
    "        # generate fake image\n",
    "        fake_B = self.G(source, z)\n",
    "\n",
    "        # discriminator loss for generated image\n",
    "        pred_fake = self.D(source, fake_B)\n",
    "        loss_G_GAN = self.criterion_GAN(pred_fake, real_label) * 0.5  # Scale down GAN loss\n",
    "\n",
    "        # l1 loss between generated and target\n",
    "        loss_G_L1 = self.criterion_L1(fake_B, target) * lambda_L1\n",
    "\n",
    "        # add perceptual loss\n",
    "        # loss_G_perceptual = config['perceptual_loss_weight'] * perceptual_loss(target, fake_B)\n",
    "        loss_G_perceptual = config['perceptual_loss_weight'] * perceptual_loss(target, fake_B, feature_extractor)\n",
    "\n",
    "\n",
    "        # KL loss with scaling\n",
    "        loss_KL = self.criterion_KL(mu, logvar) * lambda_KL\n",
    "\n",
    "        # backward cycle (cLR-GAN)\n",
    "        z_random = torch.randn(source.size(0), self.latent_dim).to(self.device)\n",
    "        fake_B_random = self.G(source, z_random)\n",
    "        mu2, logvar2 = self.E(fake_B_random)\n",
    "\n",
    "        # latent regression loss\n",
    "        loss_z_L1 = self.criterion_L1(mu2, z_random) * lambda_L1\n",
    "\n",
    "        # total generator and encoder loss\n",
    "        loss_G = loss_G_GAN + loss_G_L1 + loss_KL + loss_z_L1 + loss_G_perceptual\n",
    "\n",
    "        # check for NaN loss value\n",
    "        if not torch.isnan(loss_G):\n",
    "            loss_G.backward()\n",
    "            # clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.G.parameters(), max_grad_norm)\n",
    "            torch.nn.utils.clip_grad_norm_(self.E.parameters(), max_grad_norm)\n",
    "            self.opt_E.step()\n",
    "            self.opt_G.step()\n",
    "\n",
    "        # discriminator update\n",
    "        self.opt_D.zero_grad()\n",
    "\n",
    "        # real loss\n",
    "        pred_real = self.D(source, target)\n",
    "        loss_D_real = self.criterion_GAN(pred_real, real_label)\n",
    "\n",
    "        # fake loss (cVAE-GAN)\n",
    "        pred_fake1 = self.D(source, fake_B.detach())\n",
    "        loss_D_fake1 = self.criterion_GAN(pred_fake1, fake_label)\n",
    "\n",
    "        # fake loss (cLR-GAN)\n",
    "        pred_fake2 = self.D(source, fake_B_random.detach())\n",
    "        loss_D_fake2 = self.criterion_GAN(pred_fake2, fake_label)\n",
    "\n",
    "        # add gradient penalty\n",
    "        gp = gradient_penalty(\n",
    "            self.D,\n",
    "            torch.cat([source, target], dim=1),\n",
    "            torch.cat([source, fake_B.detach()], dim=1)\n",
    "        )\n",
    "\n",
    "        # total discriminator loss\n",
    "        loss_D = (loss_D_real + loss_D_fake1 + loss_D_fake2) * 0.5 + config['grad_penalty_weight'] * gp\n",
    "\n",
    "        # check for NaN\n",
    "        if not torch.isnan(loss_D):\n",
    "            loss_D.backward()\n",
    "            # clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.D.parameters(), max_grad_norm)\n",
    "            self.opt_D.step()\n",
    "\n",
    "        return {\n",
    "            'G_loss': loss_G.item() if not torch.isnan(loss_G) else 0,\n",
    "            'D_loss': loss_D.item() if not torch.isnan(loss_D) else 0,\n",
    "            'KL': loss_KL.item() if not torch.isnan(loss_KL) else 0,\n",
    "            'L1': loss_G_L1.item() if not torch.isnan(loss_G_L1) else 0,\n",
    "            'z_L1': loss_z_L1.item() if not torch.isnan(loss_z_L1) else 0,\n",
    "            'Grad_Penalty': gp.item() if not torch.isnan(gp) else 0,\n",
    "            'Perceptual_Loss': loss_G_perceptual.item() if not torch.isnan(loss_G_perceptual) else 0\n",
    "        }\n",
    "\n",
    "\n",
    "    def set_requires_grad(self, nets, requires_grad=False):\n",
    "        \"\"\"Set requies_grad=False for networks to avoid unnecessary computations\"\"\"\n",
    "        if not isinstance(nets, list):\n",
    "            nets = [nets]\n",
    "        for net in nets:\n",
    "            if net is not None:\n",
    "                for param in net.parameters():\n",
    "                    param.requires_grad = requires_grad\n",
    "\n",
    "\n",
    "    def encode(self, input_image):\n",
    "        mu, logvar = self.E(input_image)\n",
    "        z = self.E.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def generate(self, input_image, z):\n",
    "        return self.G(input_image, z)\n",
    "\n",
    "\n",
    "    def visualize_results(self, source, target, epoch, num_samples=4):\n",
    "        \"\"\"\n",
    "        Visualize multiple generation results from the same input image\n",
    "        Args:\n",
    "            source: Source image (ADC)\n",
    "            target: Ground truth target image (T2w)\n",
    "            epoch: Current epoch number\n",
    "            num_samples: Number of different results to generate\n",
    "        \"\"\"\n",
    "        self.E.eval()\n",
    "        self.G.eval()\n",
    "        with torch.no_grad():\n",
    "            # generate multiple outputs using different random latent codes\n",
    "            outputs = []\n",
    "            for _ in range(num_samples):\n",
    "                z = torch.randn(source.size(0), self.latent_dim).to(self.device)\n",
    "                fake = self.G(source, z)\n",
    "                outputs.append(fake)\n",
    "\n",
    "            # create visualization grid\n",
    "            # first row: [source, target]\n",
    "            # following rows: different generated outputs\n",
    "            vis_images = [source[0], target[0]]  # First row\n",
    "            for out in outputs:\n",
    "                vis_images.append(out[0])\n",
    "\n",
    "            # convert from [-1, 1] to [0, 1] range for visualization\n",
    "            vis_images = [(img + 1) / 2 for img in vis_images]\n",
    "\n",
    "            # create grid\n",
    "            image_grid = vutils.make_grid(vis_images, nrow=2, padding=2, normalize=False)\n",
    "\n",
    "            # plot\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(image_grid.cpu().numpy()[0], cmap='gray')\n",
    "            plt.title(f'Epoch {epoch}\\nTop: [Source | Target]\\nBottom: Generated Samples')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    def visualize_interpolation(self, source, target, epoch, num_steps=5):\n",
    "        \"\"\"\n",
    "        Visualize interpolation between two random latent codes\n",
    "        \"\"\"\n",
    "        self.E.eval()\n",
    "        self.G.eval()\n",
    "        with torch.no_grad():\n",
    "            # get two random latent codes\n",
    "            z1 = torch.randn(1, self.latent_dim).to(self.device)\n",
    "            z2 = torch.randn(1, self.latent_dim).to(self.device)\n",
    "\n",
    "            # interpolate between latent codes\n",
    "            alphas = torch.linspace(0, 1, num_steps)\n",
    "            interpolated = []\n",
    "\n",
    "            for alpha in alphas:\n",
    "                z = alpha * z1 + (1 - alpha) * z2\n",
    "                fake = self.G(source, z)\n",
    "                interpolated.append(fake[0])\n",
    "\n",
    "            # create visualization grid\n",
    "            vis_images = [source[0], target[0]] + interpolated\n",
    "            vis_images = [(img + 1) / 2 for img in vis_images]\n",
    "\n",
    "            # create grid\n",
    "            image_grid = vutils.make_grid(vis_images, nrow=len(vis_images), padding=2, normalize=False)\n",
    "\n",
    "            # plot\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(image_grid.cpu().numpy()[0], cmap='gray')\n",
    "            plt.title(f'Epoch {epoch}\\nLeft: [Source | Target | Interpolated Samples]')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_psnr(fake, real):\n",
    "        \"\"\"Calculate PSNR between fake and real images\"\"\"\n",
    "        mse = F.mse_loss(fake, real)\n",
    "        psnr = 20 * torch.log10(2.0 / torch.sqrt(mse))\n",
    "        return psnr.item()\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_ssim(fake, real):\n",
    "        \"\"\"Calculate SSIM between fake and real images\"\"\"\n",
    "        # implementation of SSIM calculation\n",
    "        return ssim(fake, real, data_range=2.0).item()  # 2.0 for [-1,1] range\n",
    "\n",
    "\n",
    "    def visualize_batch(self, source, target, generated, title, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize a batch of images: source, target, and generated\n",
    "        Args:\n",
    "            source: source images\n",
    "            target: target images\n",
    "            generated: list of generated images (for multiple samples)\n",
    "            title: title for the plot\n",
    "            save_path: if provided, save the plot to this path\n",
    "        \"\"\"\n",
    "        # convert tensors to PIL images\n",
    "        def tensor_to_numpy(tensor):\n",
    "            #  convert to [0, 1] (for tensor in [-1, 1] range)\n",
    "            return ((tensor.cpu().detach() + 1) / 2.0).numpy()\n",
    "\n",
    "        # create figure\n",
    "        num_samples = len(generated)\n",
    "        fig, axes = plt.subplots(2, num_samples + 2, figsize=(3*(num_samples + 2), 6))\n",
    "        plt.suptitle(title)\n",
    "\n",
    "        # plot source and target in first row\n",
    "        axes[0, 0].imshow(tensor_to_numpy(source[0])[0], cmap='gray')\n",
    "        axes[0, 0].set_title('Source (ADC)')\n",
    "        axes[0, 0].axis('off')\n",
    "\n",
    "        axes[0, 1].imshow(tensor_to_numpy(target[0])[0], cmap='gray')\n",
    "        axes[0, 1].set_title('Target (T2)')\n",
    "        axes[0, 1].axis('off')\n",
    "\n",
    "        # plot generated samples\n",
    "        for i, gen in enumerate(generated):\n",
    "            axes[0, i+2].imshow(tensor_to_numpy(gen[0])[0], cmap='gray')\n",
    "            axes[0, i+2].set_title(f'Generated {i+1}')\n",
    "            axes[0, i+2].axis('off')\n",
    "\n",
    "        # plot differences in second row\n",
    "        diff_target = np.abs(tensor_to_numpy(target[0])[0] - tensor_to_numpy(source[0])[0])\n",
    "        axes[1, 0].imshow(diff_target, cmap='hot')\n",
    "        axes[1, 0].set_title('Diff: Target-Source')\n",
    "        axes[1, 0].axis('off')\n",
    "\n",
    "        axes[1, 1].axis('off')  # empty plot for alignment\n",
    "\n",
    "        for i, gen in enumerate(generated):\n",
    "            diff_gen = np.abs(tensor_to_numpy(gen[0])[0] - tensor_to_numpy(target[0])[0])\n",
    "            axes[1, i+2].imshow(diff_gen, cmap='hot')\n",
    "            axes[1, i+2].set_title(f'Diff: Gen{i+1}-Target')\n",
    "            axes[1, i+2].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    def validate(self, val_loader, epoch, save_dir, vis=False):\n",
    "        \"\"\"Run validation with visualization\"\"\"\n",
    "        self.E.eval()\n",
    "        self.G.eval()\n",
    "        self.D.eval()\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(val_loader):\n",
    "                source = batch['A'].to(self.device)\n",
    "                target = batch['B'].to(self.device)\n",
    "\n",
    "                # generate multiple samples\n",
    "                generated_samples = []\n",
    "                for _ in range(3):  # generate 3 different samples\n",
    "                    z = torch.randn(source.size(0), self.latent_dim).to(self.device)\n",
    "                    fake_B = self.G(source, z)\n",
    "                    generated_samples.append(fake_B)\n",
    "\n",
    "                # calculate validation losses\n",
    "                loss_G_L1 = self.criterion_L1(generated_samples[0], target) * self.lambda_L1\n",
    "\n",
    "                val_losses.append({\n",
    "                    'val_L1': loss_G_L1.item(),\n",
    "                })\n",
    "\n",
    "                # visualize first few batches\n",
    "                if i == 5 and vis:  # show validation samples\n",
    "                    save_path = f\"{save_dir}/validation_epoch{epoch}_batch{i}.png\"\n",
    "                    self.visualize_batch(\n",
    "                        source, target, generated_samples,\n",
    "                        f\"Validation - Epoch {epoch}, Batch {i}\",\n",
    "                        save_path\n",
    "                    )\n",
    "\n",
    "        avg_losses = {k: sum(d[k] for d in val_losses) / len(val_losses)\n",
    "                     for k in val_losses[0].keys()}\n",
    "\n",
    "        return avg_losses\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, test_loader, num_samples=5, save_dir=None, vis = False):\n",
    "        \"\"\"Run testing with visualization\"\"\"\n",
    "        self.E.eval()\n",
    "        self.G.eval()\n",
    "\n",
    "        test_metrics = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(test_loader):\n",
    "                source = batch['A'].to(self.device)\n",
    "                target = batch['B'].to(self.device)\n",
    "\n",
    "                # generate multiple samples\n",
    "                generated_samples = []\n",
    "                for _ in range(num_samples):\n",
    "                    z = torch.randn(source.size(0), self.latent_dim).to(self.device)\n",
    "                    fake_B = self.G(source, z)\n",
    "                    generated_samples.append(fake_B)\n",
    "\n",
    "                # calculate metrics\n",
    "                metrics = {\n",
    "                    'PSNR': BicycleGAN.calculate_psnr(generated_samples[0], target),\n",
    "                    'SSIM': BicycleGAN.calculate_ssim(generated_samples[0], target),\n",
    "                    'L1': self.criterion_L1(generated_samples[0], target).item()\n",
    "                }\n",
    "                test_metrics.append(metrics)\n",
    "\n",
    "                # visualize\n",
    "                if save_dir and vis and (i == random.choice([s for s in range(len(test_loader))])) :  # Show 2 test samples\n",
    "\n",
    "                    random_index = random.randint(0, num_samples - 1)  # random index between 0 and num_samples - 1\n",
    "                    random_sample = generated_samples[random_index]\n",
    "\n",
    "                    save_path = f\"{save_dir}/test_sample{i}_random.png\"\n",
    "                    self.visualize_batch(\n",
    "                        source, target, [random_sample],  # pass the random sample\n",
    "                        f\"Test Sample {i} (Random Selection)\",\n",
    "                        save_path\n",
    "                    )\n",
    "\n",
    "        avg_metrics = {k: sum(d[k] for d in test_metrics) / len(test_metrics)\n",
    "                      for k in test_metrics[0].keys()}\n",
    "\n",
    "        return avg_metrics\n",
    "\n",
    "    def get_current_lrs(self):\n",
    "        return {\n",
    "            'lr_G': self.opt_G.param_groups[0]['lr'],\n",
    "            'lr_E': self.opt_E.param_groups[0]['lr'],\n",
    "            'lr_D': self.opt_D.param_groups[0]['lr']\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:41.991957Z",
     "iopub.status.busy": "2024-12-10T19:32:41.991591Z",
     "iopub.status.idle": "2024-12-10T19:32:42.011880Z",
     "shell.execute_reply": "2024-12-10T19:32:42.011078Z",
     "shell.execute_reply.started": "2024-12-10T19:32:41.991918Z"
    },
    "id": "Cs7eBZ6kW1fl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_model_summaries(model):\n",
    "    \"\"\"\n",
    "    Print detailed summaries for all components of the BicycleGAN model in torchsummary style\n",
    "    \"\"\"\n",
    "    def calculate_conv_output_shape(input_size, kernel_size, stride, padding):\n",
    "        return ((input_size + 2 * padding - kernel_size) // stride) + 1\n",
    "\n",
    "    def calculate_convt_output_shape(input_size, kernel_size, stride, padding):\n",
    "        return (input_size - 1) * stride - 2 * padding + kernel_size\n",
    "\n",
    "    def print_layer_summary(layer_name, input_shape, output_shape, params):\n",
    "        print(f\"| {layer_name:<20} | {str(input_shape):<15} | {str(output_shape):<15} | {params:>8} |\")\n",
    "\n",
    "    def print_network_header(name):\n",
    "        print(f\"\\n{name} Network:\")\n",
    "        print(\"=\" * 71)\n",
    "        print(f\"| {'Layer':<20} | {'Input Shape':<15} | {'Output Shape':<15} | {'Params':>8} |\")\n",
    "        print(\"=\" * 71)\n",
    "\n",
    "    print(\"\\n===================== BicycleGAN Model Summary =========================\")\n",
    "\n",
    "    # encoder Summary\n",
    "    print_network_header(\"Encoder\")\n",
    "    curr_size = 256\n",
    "    in_channels = 1\n",
    "\n",
    "    for name, layer in model.E.named_modules():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            output_size = calculate_conv_output_shape(curr_size, layer.kernel_size[0],\n",
    "                                                    layer.stride[0], layer.padding[0])\n",
    "            params = layer.in_channels * layer.out_channels * layer.kernel_size[0] * layer.kernel_size[1]\n",
    "            if layer.bias is not None:\n",
    "                params += layer.out_channels\n",
    "\n",
    "            print_layer_summary(\n",
    "                f\"Conv2d-{layer.out_channels}\",\n",
    "                (in_channels, curr_size, curr_size),\n",
    "                (layer.out_channels, output_size, output_size),\n",
    "                params\n",
    "            )\n",
    "            curr_size = output_size\n",
    "            in_channels = layer.out_channels\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            params = 2 * layer.num_features \n",
    "            print_layer_summary(\n",
    "                \"BatchNorm2d\",\n",
    "                (in_channels, curr_size, curr_size),\n",
    "                (in_channels, curr_size, curr_size),\n",
    "                params\n",
    "            )\n",
    "\n",
    "    # linear layers for mu and logvar\n",
    "    fc_input_size = 512 * 4 * 4\n",
    "    print_layer_summary(\n",
    "        \"Linear-mu\",\n",
    "        (fc_input_size,),\n",
    "        (model.latent_dim,),\n",
    "        fc_input_size * model.latent_dim + model.latent_dim\n",
    "    )\n",
    "    print_layer_summary(\n",
    "        \"Linear-logvar\",\n",
    "        (fc_input_size,),\n",
    "        (model.latent_dim,),\n",
    "        fc_input_size * model.latent_dim + model.latent_dim\n",
    "    )\n",
    "\n",
    "    # generator summary\n",
    "    print_network_header(\"Generator\")\n",
    "    curr_size = 256\n",
    "    in_channels = 1\n",
    "\n",
    "    # generator's encoder path\n",
    "    for block in model.G.down_blocks:\n",
    "        for layer in block:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                output_size = calculate_conv_output_shape(curr_size, layer.kernel_size[0],\n",
    "                                                        layer.stride[0], layer.padding[0])\n",
    "                params = layer.in_channels * layer.out_channels * layer.kernel_size[0] * layer.kernel_size[1]\n",
    "                if layer.bias is not None:\n",
    "                    params += layer.out_channels\n",
    "\n",
    "                print_layer_summary(\n",
    "                    f\"Conv2d-{layer.out_channels}\",\n",
    "                    (in_channels, curr_size, curr_size),\n",
    "                    (layer.out_channels, output_size, output_size),\n",
    "                    params\n",
    "                )\n",
    "                curr_size = output_size\n",
    "                in_channels = layer.out_channels\n",
    "\n",
    "    # generator's decoder path\n",
    "    for block in model.G.up_blocks:\n",
    "        for layer in block:\n",
    "            if isinstance(layer, nn.ConvTranspose2d):\n",
    "                output_size = calculate_convt_output_shape(curr_size, layer.kernel_size[0],\n",
    "                                                         layer.stride[0], layer.padding[0])\n",
    "                params = layer.in_channels * layer.out_channels * layer.kernel_size[0] * layer.kernel_size[1]\n",
    "                if layer.bias is not None:\n",
    "                    params += layer.out_channels\n",
    "\n",
    "                print_layer_summary(\n",
    "                    f\"ConvTranspose2d-{layer.out_channels}\",\n",
    "                    (in_channels, curr_size, curr_size),\n",
    "                    (layer.out_channels, output_size, output_size),\n",
    "                    params\n",
    "                )\n",
    "                curr_size = output_size\n",
    "                in_channels = layer.out_channels\n",
    "\n",
    "    # discriminator summary\n",
    "    print_network_header(\"Discriminator\")\n",
    "    curr_size = 256\n",
    "    in_channels = 2  # concatenated source and target\n",
    "\n",
    "    for name, layer in model.D.named_modules():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            output_size = calculate_conv_output_shape(curr_size, layer.kernel_size[0],\n",
    "                                                    layer.stride[0], layer.padding[0])\n",
    "            params = layer.in_channels * layer.out_channels * layer.kernel_size[0] * layer.kernel_size[1]\n",
    "            if layer.bias is not None:\n",
    "                params += layer.out_channels\n",
    "\n",
    "            print_layer_summary(\n",
    "                f\"Conv2d-{layer.out_channels}\",\n",
    "                (in_channels, curr_size, curr_size),\n",
    "                (layer.out_channels, output_size, output_size),\n",
    "                params\n",
    "            )\n",
    "            curr_size = output_size\n",
    "            in_channels = layer.out_channels\n",
    "\n",
    "    # Total parameters\n",
    "    total_params = (sum(p.numel() for p in model.E.parameters()) +\n",
    "                   sum(p.numel() for p in model.G.parameters()) +\n",
    "                   sum(p.numel() for p in model.D.parameters()))\n",
    "    print(\"\\n\" + \"=\" * 71)\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:42.013187Z",
     "iopub.status.busy": "2024-12-10T19:32:42.012930Z",
     "iopub.status.idle": "2024-12-10T19:32:42.760485Z",
     "shell.execute_reply": "2024-12-10T19:32:42.759639Z",
     "shell.execute_reply.started": "2024-12-10T19:32:42.013161Z"
    },
    "id": "xDr72TneeDkQ",
    "outputId": "2059c705-13c0-46db-e390-7001043b8582",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model summary\n",
    "model = BicycleGAN()\n",
    "print_model_summaries(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG7JEALve2lH"
   },
   "source": [
    "# Saving paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:42.762181Z",
     "iopub.status.busy": "2024-12-10T19:32:42.761792Z",
     "iopub.status.idle": "2024-12-10T19:32:42.767770Z",
     "shell.execute_reply": "2024-12-10T19:32:42.766897Z",
     "shell.execute_reply.started": "2024-12-10T19:32:42.762143Z"
    },
    "id": "HHhVmVlnesZS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_path = config[\"output_dir\"]\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "save_checkp_path = f'{config[\"output_dir\"]}/checkpoints'\n",
    "os.makedirs(save_checkp_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0obtm7y__-t"
   },
   "source": [
    "# Wandb Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:42.769347Z",
     "iopub.status.busy": "2024-12-10T19:32:42.768971Z",
     "iopub.status.idle": "2024-12-10T19:32:45.077557Z",
     "shell.execute_reply": "2024-12-10T19:32:45.076665Z",
     "shell.execute_reply.started": "2024-12-10T19:32:42.769309Z"
    },
    "id": "TdPClnMx__M8",
    "outputId": "3c7b18a2-36ec-4f25-b69e-61547082a4db",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:45.078909Z",
     "iopub.status.busy": "2024-12-10T19:32:45.078525Z",
     "iopub.status.idle": "2024-12-10T19:32:46.747844Z",
     "shell.execute_reply": "2024-12-10T19:32:46.746884Z",
     "shell.execute_reply.started": "2024-12-10T19:32:45.078884Z"
    },
    "id": "U82EswSbFaMw",
    "outputId": "986e5396-1c5e-4be2-fc0c-988179145bac",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancistate\u001b[0m (\u001b[33mGroupXV\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241210_193245-02mc7voz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/GroupXV/project-ablations/runs/02mc7voz' target=\"_blank\">bgan-sa-perc-gradpen-v11</a></strong> to <a href='https://wandb.ai/GroupXV/project-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/GroupXV/project-ablations' target=\"_blank\">https://wandb.ai/GroupXV/project-ablations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/GroupXV/project-ablations/runs/02mc7voz' target=\"_blank\">https://wandb.ai/GroupXV/project-ablations/runs/02mc7voz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    name = config[\"run_name\"],\n",
    "    reinit = True,\n",
    "    # id = ###\n",
    "    # resume = \"must\"\n",
    "    project = \"project-ablations\",\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_H-DUBHlh_h"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:46.749486Z",
     "iopub.status.busy": "2024-12-10T19:32:46.749060Z",
     "iopub.status.idle": "2024-12-10T19:32:46.771125Z",
     "shell.execute_reply": "2024-12-10T19:32:46.770260Z",
     "shell.execute_reply.started": "2024-12-10T19:32:46.749436Z"
    },
    "id": "a2X2ZiU_mJnz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, test_loader, num_epochs, save_path):\n",
    "    \"\"\"Complete training process with visualizations\"\"\"\n",
    "    losses_history = []\n",
    "    val_history = []\n",
    "    test_metrics = None\n",
    "    best_val_loss = float('inf')\n",
    "    best_psnr = 2.6692\n",
    "    patience = 15 \n",
    "    #epochs until early stop if no improvement in\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Create directories for saving visualizations\n",
    "    vis_dir = os.path.join(save_path, 'visualizations')\n",
    "    val_dir = os.path.join(vis_dir, 'validation')\n",
    "    test_dir = os.path.join(vis_dir, 'test')\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_losses = []\n",
    "        model.G.train()\n",
    "        model.E.train()\n",
    "        model.D.train()\n",
    "\n",
    "        all_batches = []\n",
    "\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "            for batch in train_loader:\n",
    "                source = batch['A'].to(model.device)\n",
    "                target = batch['B'].to(model.device)\n",
    "\n",
    "                # store batch for potential random visualization\n",
    "                all_batches.append((source, target))\n",
    "\n",
    "                # training step\n",
    "                losses = model.train_step(source, target)\n",
    "\n",
    "                # skip this batch if NaN losses were computed\n",
    "                if losses['G_loss'] == 0 and losses['D_loss'] == 0:\n",
    "                    print(f\"Skipping batch  due to NaN losses\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                epoch_losses.append(losses)\n",
    "                pbar.set_postfix(**losses)\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "        # visualize a random batch after completing the epoch\n",
    "        if epoch % 5 == 0 and all_batches:\n",
    "            random_batch = random.choice(all_batches)  # Select a random batch from the epoch\n",
    "            source, target = random_batch\n",
    "            model.visualize_results(source, target, epoch)\n",
    "\n",
    "        # average epoch losses\n",
    "        if epoch_losses:  # Only if we have valid losses\n",
    "            avg_losses = {k: sum(d[k] for d in epoch_losses) / len(epoch_losses)\n",
    "                          for k in epoch_losses[0].keys()}\n",
    "            wandb.log(avg_losses)\n",
    "            losses_history.append(avg_losses)\n",
    "\n",
    "        # print epoch summary\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - Avg G_loss: {avg_losses['G_loss']:.4f}, \"\n",
    "              f\"Avg D_loss: {avg_losses['D_loss']:.4f}, LRs: {model.get_current_lrs()}\")\n",
    "\n",
    "        # run validation\n",
    "        val_losses = model.validate(val_loader, epoch, val_dir, vis = False) #vis=(epoch % 10 == 0))\n",
    "        val_history.append(val_losses)\n",
    "        wandb.log(val_losses)\n",
    "\n",
    "        # # Periodic test evaluation\n",
    "        # if epoch % 5 == 0:\n",
    "        #     test_metrics = model.test(test_loader, num_samples=1, save_dir=test_dir, vis=True) # change t\n",
    "        # else:\n",
    "        #     test_metrics = model.test(test_loader, num_samples=1, save_dir=test_dir)\n",
    "\n",
    "        test_metrics = model.test(test_loader, num_samples=1, save_dir=test_dir)\n",
    "        # print test metrics summary\n",
    "        print(f\"Test Metrics - Epoch {epoch}:\")\n",
    "        for k, v in test_metrics.items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "        wandb.log(test_metrics)\n",
    "\n",
    "        # update learning rate schedulers using test metrics (e.g., PSNR)\n",
    "        model.scheduler_G.step(-test_metrics['PSNR'])\n",
    "        model.scheduler_E.step(-test_metrics['PSNR'])\n",
    "        model.scheduler_D.step(-test_metrics['PSNR'])\n",
    "\n",
    "        # save the best model based on PSNR\n",
    "        if test_metrics['PSNR'] > best_psnr:\n",
    "            best_psnr = test_metrics['PSNR']\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'G_state_dict': model.G.state_dict(),\n",
    "                'E_state_dict': model.E.state_dict(),\n",
    "                'D_state_dict': model.D.state_dict(),\n",
    "                'opt_G_state_dict': model.opt_G.state_dict(),\n",
    "                'opt_E_state_dict': model.opt_E.state_dict(),\n",
    "                'opt_D_state_dict': model.opt_D.state_dict(),\n",
    "                'losses': losses_history,\n",
    "                'val_losses': val_history,\n",
    "                'test_metrics': test_metrics\n",
    "            }\n",
    "            torch.save(checkpoint, f\"{save_path}/bicycle_gan_best_psnr.pt\")\n",
    "            wandb.save(f\"{save_path}/bicycle_gan_best_psnr.pt\")\n",
    "            print(\"Saved best PSNR model\")\n",
    "\n",
    "            patience_counter = 0\n",
    "            \n",
    "\n",
    "        # # Early stopping check\n",
    "        # if val_losses['val_L1'] < best_val_loss:\n",
    "        #     best_val_loss = val_losses['val_L1']\n",
    "        #     patience_counter = 0\n",
    "\n",
    "            # # Save best validation model\n",
    "            # checkpoint = {\n",
    "            #     'epoch': epoch,\n",
    "            #     'G_state_dict': model.G.state_dict(),\n",
    "            #     'E_state_dict': model.E.state_dict(),\n",
    "            #     'D_state_dict': model.D.state_dict(),\n",
    "            #     'opt_G_state_dict': model.opt_G.state_dict(),\n",
    "            #     'opt_E_state_dict': model.opt_E.state_dict(),\n",
    "            #     'opt_D_state_dict': model.opt_D.state_dict(),\n",
    "            #     'losses': losses_history,\n",
    "            #     'val_losses': val_history\n",
    "            # }\n",
    "            # torch.save(checkpoint, f\"{save_path}/bicycle_gan_best.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "        # Save regular checkpoint every 10 epochs\n",
    "        # if epoch % 10 == 0:\n",
    "        #     checkpoint = {\n",
    "        #         'epoch': epoch,\n",
    "        #         'G_state_dict': model.G.state_dict(),\n",
    "        #         'E_state_dict': model.E.state_dict(),\n",
    "        #         'D_state_dict': model.D.state_dict(),\n",
    "        #         'opt_G_state_dict': model.opt_G.state_dict(),\n",
    "        #         'opt_E_state_dict': model.opt_E.state_dict(),\n",
    "        #         'opt_D_state_dict': model.opt_D.state_dict(),\n",
    "        #         'losses': losses_history,\n",
    "        #         'val_losses': val_history\n",
    "        #     }\n",
    "        #     torch.save(checkpoint, f\"{save_path}/bicycle_gan_epoch_{epoch}.pt\")\n",
    "\n",
    "    # return losses_history, val_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GHoJjojgBhw"
   },
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:46.772505Z",
     "iopub.status.busy": "2024-12-10T19:32:46.772237Z",
     "iopub.status.idle": "2024-12-10T19:32:48.933078Z",
     "shell.execute_reply": "2024-12-10T19:32:48.932312Z",
     "shell.execute_reply.started": "2024-12-10T19:32:46.772473Z"
    },
    "id": "J5WzjHNHNqpH",
    "outputId": "4e3217c8-d0e6-4dd6-df98-455f11da4d07",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# losses, val_losses = train(model, train_loader, val_loader, test_loader,\n",
    "#                          num_epochs=config['epochs'], save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T19:32:48.934700Z",
     "iopub.status.busy": "2024-12-10T19:32:48.934346Z"
    },
    "id": "UcqJ5sOO3y9Q",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train(model, train_loader, val_loader, test_loader,\n",
    "      num_epochs=config['epochs'], save_path=save_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6258153,
     "sourceId": 10139781,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6271917,
     "sourceId": 10157874,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
